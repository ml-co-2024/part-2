{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook setup\n",
    "# ============================================================\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Control figure size\n",
    "figsize=(14, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Constrained ML via Data Projection\n",
    "\n",
    "To be prepared is half of the victory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's Face It\n",
    "\n",
    "**9 times out of 10, we are dealing with _supervised_ learning**\n",
    "\n",
    "Meaning a basic training problem is in the form:\n",
    "\n",
    "$$\n",
    "\\mathop{\\rm argmin}_\\theta \\{ L(\\hat{y}, y) \\mid \\hat{y} = f(x; \\theta) \\}\n",
    "$$\n",
    "\n",
    "* Where the ground truth vector $y= \\{y_i\\}_{i=1}^m$ appears in the loss expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The corresponding constrained version is:**\n",
    "\n",
    "$$\n",
    "\\mathop{\\rm argmin}_\\theta \\{ L(\\hat{y}, y) \\mid \\hat{y} = f(x; \\theta), g(\\hat{y}) \\leq 0 \\}\n",
    "$$\n",
    "\n",
    "* The presence of the constraints usually contrasts with the loss function\n",
    "* ...Resulting in a change in the optimal parameter vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=big>\n",
    "But what if it doesn't?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redundant Constraints\n",
    "\n",
    "**Let's assume the constraint are _aligned_ with the loss**\n",
    "\n",
    "In this case, we can expect:\n",
    "\n",
    "$$\n",
    "\\mathop{\\rm argmin}_\\theta L(\\hat{y}, y) \\simeq \\mathop{\\rm argmin}_\\theta \\{ L(\\hat{y}, y) \\mid g(\\hat{y}) \\leq 0 \\} \\quad \\text{ with: } \\hat{y} = f(x; \\theta)\n",
    "$$\n",
    "\n",
    "* Since the constraints are somewhat redundant\n",
    "* ...We can expect the loss function to _correlate_ with constraint violation\n",
    "\n",
    "As a result, the two optimal parameter vector should be similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **In practice, solving the unconstrained problem approximately solves the constrained one**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enforcing Constraints via Preprocessing\n",
    "\n",
    "**Based on this idea, we can try to enforce the constraints via pre-processing**\n",
    "\n",
    "First, we compute an adjusted target vector:\n",
    "\n",
    "$$\n",
    "z^* = \\mathop{\\rm argmin}_{z} \\{ L(z, y) \\mid g(z) \\leq 0 \\}\n",
    "$$\n",
    "\n",
    "...Then we solve an unconstrained supervised learning problem:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\mathop{\\rm argmin}_{\\theta} \\{ L(\\hat{y}, z^*) \\mid \\hat{y} = f(x; \\theta) \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In terms of accuracy, we can expect $\\theta^*$ to be a good parameter vector**\n",
    "\n",
    "* In particular, if $L$ is a metric or quasimetric\n",
    "* ...Then we have $L(f(x; \\theta^*), y) \\leq L(f(x; \\theta^*), z^*) + L(z^*, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enforcing Constraints via Preprocessing\n",
    "\n",
    "**Based on this idea, we can try to enforce the constraints via pre-processing**\n",
    "\n",
    "First, we compute an adjusted target vector:\n",
    "\n",
    "$$\n",
    "z^* = \\mathop{\\rm argmin}_{z} \\{ L(z, y) \\mid g(z) \\leq 0 \\}\n",
    "$$\n",
    "\n",
    "...Then we solve an unconstrained supervised learning problem:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\mathop{\\rm argmin}_{\\theta} \\{ L(\\hat{y}, z^*) \\mid \\hat{y} = f(x; \\theta) \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In terms of constraint satisfaction:**\n",
    "\n",
    "* $z^*$ will satisfy the constrains by construction\n",
    "* $f(x; \\theta^*)$ will be approximately feasible iff $L$ is correlated with $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocessing and Projection\n",
    "\n",
    "**It's useful to inspect our preprocessing step in detail**\n",
    "\n",
    "$$\n",
    "z^* = \\mathop{\\rm argmin}_{z} \\{ L(z, y) \\mid g(z) \\leq 0 \\}\n",
    "$$\n",
    "\n",
    "* This can be understood as a form of _projection_\n",
    "* ...And compared with the proximal operator for the indicator function $I_g$\n",
    "\n",
    "$$\n",
    "\\mathop{\\bf prox}_{g}(y) = \\mathop{\\rm argmin}_{z} \\{ I_{g}(z) + \\|z - y\\|_2^2\\}\n",
    "$$\n",
    "\n",
    "The main difference is the use of $L$ on one side and $\\|\\cdot\\|_2^2$ on the other\n",
    "\n",
    "* In case of an MSE loss, there is no difference at all!\n",
    "* For other loss functions, we still have a strong analogy\n",
    "\n",
    "**For this reason, we'll call this approach \"data projection\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Analysis of the Approach\n",
    "\n",
    "**Unlike in training-time Lagrangians, here there's an inherent approximation**\n",
    "\n",
    "* So, it's difficult to provide guarantees\n",
    "* What we can expect is _approximate_ constraint satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The approach makes use of a Monte-Carlo approximation**\n",
    "\n",
    "* So be on the lookout for overfitting issues\n",
    "* ...And always check constraint satisfaction on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The computational effort should be considered**\n",
    "\n",
    "* We need to solve a large scale constrained optimization problem\n",
    "* ...But one that is defined entirely in target space\n",
    "* ...And therefore typically with a nice structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Analysis of the Approach\n",
    "\n",
    "**The approach is well suited to deal with _relational constraints_**\n",
    "\n",
    "* This is the case since we can access (in theory) all data at the same time\n",
    "* The best target are _distribution_ constraints\n",
    "* ...Since for them approximate satisfaction is typically the best we can do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Data projection has wide compatibility**\n",
    "\n",
    "* Since we modify directly the dataset\n",
    "* ...We can then use _any_ supervised learning technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=big>\n",
    "Let's try to formulate the projection problem for our fairness use case\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projection for our Fairness Case Study\n",
    "\n",
    "**We have a regression problem and our loss function is the MSE**\n",
    "\n",
    "$$\n",
    "\\mathop{\\rm argmin}_{\\theta} \\| z - y \\|_2^2\n",
    "$$\n",
    "\n",
    "* $z$ is the projected target vector, $y$ the original one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**We have a constratint on the DIDI index, with a single protected attribute**\n",
    "\n",
    "...Which can be understood as the sum of multiple deviation\n",
    "\n",
    "$$\n",
    "\\sum_{v \\in D} \\left|\\frac{1}{m} \\sum_{i=1}^m z_i - \\frac{1}{|X_{v}|} \\sum_{i =1}^m X_{v,i} z_{i}\\right| \\leq \\varepsilon\n",
    "$$\n",
    "\n",
    "* Every protected group is associated to a value $v$ of the sensitive attribute\n",
    "* Every entry $X_{v,i}$ is 1 iff the $i$-th example is part of group $v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projection for our Fairness Case Study\n",
    "\n",
    "**The objective can be stated directly, but the constraint takes more effort**\n",
    "\n",
    "We start by viewing the DIDI as a sum of deviations:\n",
    "\n",
    "$$\n",
    "\\sum_{v \\in D} d_v \\leq \\varepsilon\n",
    "$$\n",
    "\n",
    "* $d_v$ represents the L1 norm between the average target for group $v$\n",
    "* ...And the overal target average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**So it makes sense to model such averages through additional variables**\n",
    "\n",
    "$$\n",
    "\\bar{z} = \\frac{1}{m} 1^T z \\quad \\text{ and } \\quad \\bar{z}_v = \\frac{1}{\\|X_{v}\\|_1} X_v z \\quad \\forall v \\in D\n",
    "$$\n",
    "\n",
    "Both averages are defined by linear expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projection for our Fairness Case Study\n",
    "\n",
    "**Based on the averages, we can model the L1 norms**\n",
    "\n",
    "$$\n",
    "d_v = |\\bar{z}_v - \\bar{z}| \\quad \\forall v \\in D\n",
    "$$\n",
    "\n",
    "There are non-linear expression, but can be linearized as:\n",
    "\n",
    "$$\n",
    "d_v \\geq \\bar{z}_v - \\bar{z} \\quad \\forall v \\in D \\\\\n",
    "d_v \\geq -\\bar{z}_v + \\bar{z} \\quad \\forall v \\in D \n",
    "$$\n",
    "\n",
    "* The inequalities do not defined $d_v$ univocally\n",
    "* ...Rather, the only provide a _lower bound_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**But that is still enough!**\n",
    "\n",
    "* Due to our constrain, keeping $d_v$ is always preferable\n",
    "* ...Meaning that optimization itself will keep the constraints tight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projection for our Fairness Case Study\n",
    "\n",
    "**Overall, the projection problem can be formulated as:**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathop{\\rm argmin}_{\\hat{y}} & \\|y - z\\|_2 \\\\\n",
    "\\text{subject to: } & \\bar{y} = \\frac{1}{m} {\\bf 1}^T z \\\\\n",
    "& \\bar{y}_v = \\frac{1}{\\|X_{v}\\|_1} X_{v} z & \\forall v \\in D \\\\\n",
    "& d_v \\geq \\bar{y} - \\bar{y}_v & \\forall v \\in D \\\\\n",
    "& d_v \\geq -(\\bar{y} - \\bar{y}_v) & \\forall v \\in D \\\\\n",
    "& \\sum_{v \\in D} d_v \\leq \\varepsilon\n",
    "\\end{align}$$\n",
    "\n",
    "...Which is a _quadratic program_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=big>\n",
    "    To shake things up, we'll solve this problem via a new approach!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "rise": {
   "center": false,
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
